###
# Model Configuration: Llama 3.2 1B
###

base_model: unsloth/Llama-3.2-1B
sequence_len: 4096

# base model weight quantization
load_in_8bit: true

# attention implementation
flash_attention: true

# finetuned adapter config
adapter: lora
lora_model_dir:
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_linear: true
lora_fan_in_fan_out:
lora_modules_to_save:
    - embed_tokens
    - lm_head

###
# Dataset Configuration: sqlqa
###

datasets:
    - path: data.jsonl
      ds_type: json
      type:
          field_instruction: question
          field_output: answer
          format: |-
              [INST] Solve the following math problem.
              {instruction} [/INST]

# dataset formatting config
tokens:
    - "[INST]"
    - " [/INST]"

special_tokens:
    pad_token: <|end_of_text|>

val_set_size: 0.05

###
# Training Configuration
###

seed: 117

# optimizer config
optimizer: adamw_bnb_8bit
learning_rate: 0.0001
lr_scheduler: cosine
num_epochs: 4
micro_batch_size: 32
gradient_accumulation_steps: 1
warmup_steps: 10

# axolotl saving config
dataset_prepared_path: last_run_prepared
output_dir: ./lora-out

# logging and eval config
logging_steps: 1
eval_steps: 0.05

# training performance optimization config
bf16: auto
tf32: false
gradient_checkpointing: true

###
# Miscellaneous Configuration
###

strict: false
local_rank:

# wandb config
wandb_project: dynamo
wandb_watch: gradients

<html>
    <head>
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>

        <link rel="shortcut icon" href="images/icon.ico" />
        <style type="text/css">
            body {
            	background-color: #f5f9ff;
            }

            /* Hide both math displays initially, will display based on JS detection */
             .mathjax-mobile, .mathml-non-mobile { display: none; }

             /* Show the MathML content by default on non-mobile devices */
             .show-mathml .mathml-non-mobile { display: block; }
             .show-mathjax .mathjax-mobile { display: block; }

            .content-margin-container {
            	display: flex;
            	width: 100%; /* Ensure the container is full width */
            	justify-content: left; /* Horizontally centers the children in the container */
            	align-items: center;  /* Vertically centers the children in the container */
            }
            .main-content-block {
            	width: 70%; /* Change this percentage as needed */
               max-width: 1100px; /* Optional: Maximum width */
            	background-color: #fff;
            	border-left: 1px solid #DDD;
            	border-right: 1px solid #DDD;
            	padding: 8px 8px 8px 8px;
            	font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;#"Avenir";
            }
            .margin-left-block {
            		font-size: 14px;
            		width: 15%; /* Change this percentage as needed */
            		max-width: 130px; /* Optional: Maximum width */
            		position: relative;
            		margin-left: 10px;
            		text-align: left;
            		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;#"Avenir";
            		padding: 5px;
            }
            .margin-right-block {
            		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;#"Avenir";
            		font-size: 14px;
            		width: 25%; /* Change this percentage as needed */
            		max-width: 256px; /* Optional: Maximum width */
            		position: relative;
            		text-align: left;
            		padding: 10px;  /* Optional: Adds padding inside the caption */
            }

            img {
            		max-width: 100%; /* Make sure it fits inside the container */
            		height: auto;
            		display: block;
            		margin: auto;
            }
            .my-video {
            		max-width: 100%; /* Make sure it fits inside the container */
            		height: auto;
            		display: block;
            		margin: auto;
            }
            /* Hide both video displays initially, will display based on JS detection */
             .vid-mobile, .vid-non-mobile { display: none; }

             /* Show the video content by default on non-mobile devices */
             .show-vid-mobile .vid-mobile { display: block; }
             .show-vid-non-mobile .vid-non-mobile { display: block; }

            a:link,a:visited
            {
            	color: #0e7862; /*#1367a7;*/
            	text-decoration: none;
            }
            a:hover {
            	color: #24b597; /*#208799;*/
            }

            h1 {
            	font-size: 18px;
            	margin-top: 4px;
            	margin-bottom: 10px;
            }

            table.header {
               font-weight: 300;
               font-size: 17px;
               flex-grow: 1;
            	width: 70%;
               max-width: calc(100% - 290px); /* Adjust according to the width of .paper-code-tab */
            }
            table td, table td * {
                vertical-align: middle;
                position: relative;
            }
            table.paper-code-tab {
                flex-shrink: 0;
                margin-left: 8px;
                margin-top: 8px;
                padding: 0px 0px 0px 8px;
                width: 290px;
                height: 150px;
            }

            .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
            	box-shadow:
            	        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            	        5px 5px 0 0px #fff, /* The second layer */
            	        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            	        10px 10px 0 0px #fff, /* The third layer */
            	        10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
            	margin-top: 5px;
            	margin-left: 10px;
            	margin-right: 30px;
            	margin-bottom: 5px;
            }

            hr {
               height: 1px; /* Sets the height of the line to 1 pixel */
               border: none; /* Removes the default border */
               background-color: #DDD; /* Sets the line color to black */
             }

            div.hypothesis {
            	width: 80%;
            	background-color: #EEE;
            	border: 1px solid black;
            	border-radius: 10px;
            	-moz-border-radius: 10px;
            	-webkit-border-radius: 10px;
            	font-family: Courier;
            	font-size: 18px;
            	text-align: center;
            	margin: auto;
            	padding: 16px 16px 16px 16px;
            }

            div.citation {
               font-size: 0.8em;
               background-color:#fff;
               padding: 10px;
            	height: 200px;
             }

            .fade-in-inline {
            	position: absolute;
            	text-align: center;
            	margin: auto;
            	-webkit-mask-image: linear-gradient(to right,
            																		transparent 0%,
            																		transparent 40%,
            																		black 50%,
            																		black 90%,
            																		transparent 100%);
            	mask-image: linear-gradient(to right,
            															transparent 0%,
            															transparent 40%,
            															black 50%,
            															black 90%,
            															transparent 100%);
            	-webkit-mask-size: 8000% 100%;
            	mask-size: 8000% 100%;
            	animation-name: sweepMask;
            	animation-duration: 4s;
            	animation-iteration-count: infinite;
            	animation-timing-function: linear;
            	animation-delay: -1s;
            }

            .fade-in2-inline {
            		animation-delay: 1s;
            }

            .inline-div {
            		position: relative;
                display: inline-block; /* Makes both the div and paragraph inline-block elements */
                vertical-align: top; /* Aligns them at the top, you can adjust this to middle, bottom, etc., based on your needs */
                width: 50px; /* Optional: Adds space between the div and the paragraph */
            }
        </style>

        <title>DynaMO</title>
        <meta property="og:title" content="Dynamo" />
        <meta charset="UTF-8" />
    </head>

    <body>
        <div class="content-margin-container">
            <div class="margin-left-block"></div>
            <div class="main-content-block">
                <table class="header" align="left">
                    <tr>
                        <td colspan="4">
                            <span
                                style="
                                    font-size: 32px;
                                    font-family: 'Courier New', Courier,
                                        monospace; /* Adds fallbacks */
                                "
                                >Dynamic Mixture Optimization (DynaMO)</span
                            >
                        </td>
                    </tr>
                    <tr>
                        <td align="left">
                            <span style="font-size: 17px"
                                ><a href="your_website"
                                    >Abhay Bestrapalli</a
                                ></span
                            >
                        </td>
                        <td align="left">
                            <span style="font-size: 17px"
                                ><a href="your_partner's_website"
                                    >Gilford Ting</a
                                ></span
                            >
                        </td>
                    </tr>

                    <tr>
                        <td colspan="4" align="left">
                            <span style="font-size: 18px"
                                >Final project for 6.7960, MIT</span
                            >
                        </td>
                    </tr>
                </table>
            </div>
            <div class="margin-right-block"></div>
        </div>

        <div class="content-margin-container" id="intro">
            <div class="margin-left-block">
                <!-- table of contents here -->
                <div
                    style="
                        position: fixed;
                        max-width: inherit;
                        top: max(20%, 120px);
                    "
                >
                    <b style="font-size: 16px">Outline</b><br /><br />
                    <a href="#intro">Introduction</a><br /><br />
                    <a href="#does_x_do_y">Does X do Y?</a><br /><br />
                    <a href="#implications_and_limitations"
                        >Implications and limitations</a
                    ><br /><br />
                </div>
            </div>
            <div class="main-content-block">
                <!--You can embed an image like this:-->
                <img src="./images/your_image_here.png" width="512px" />
            </div>
            <div class="margin-right-block">Caption for the image.</div>
        </div>

        <div class="content-margin-container" id="intro">
            <div class="margin-left-block"></div>
            <div class="main-content-block">
                <h1 style="font-size: 32px">Introduction</h1>
                <hr style="border-top: 1px solid #cccccc" />
                Large language models (LLMs) have revolutionized natural
                language processing by harnessing vast and diverse datasets
                during training. Modern LLMs are trained on trillions of tokens
                of data over several stages. These datasets span an array of
                domains and subdomains — from programming languages like Python
                and Java to specialized fields like legal documents, scientific
                literature, and creative writing. However, this diversity
                introduces a crucial challenge:
                <b>determining the optimal composition of training data.</b>
                Simply training on all available data can lead to
                inefficiencies, such as overemphasizing certain tasks or domains
                at the expense of others, and may fail to leverage the full
                potential of the model. For instance, one direct way this
                heuristic is universally applied is by splitting training into
                pre-training, with simpler and more diverse data, and
                fine-tuning, with task-specific and high-quality data.
                <br /><br />
                This challenge of choosing data mixtures is further compounded
                by insights from
                <b>neural scaling laws</b> which emphasize that both model size
                and dataset size must be scaled in tandem for optimal
                performance. It is, therefore, important, to these findings
                underscore the importance of deliberate, empirical strategies in
                curating training data mixtures rather than relying on ad hoc
                methods. The question of how to systematically identify and
                fine-tune the ideal blend of training data is thus an open and
                pressing problem in the field. <br /><br />

                Our work introduces
                <b>DynaMO (Dynamic Mixture Optimization)</b> — a framework that
                dynamically adjusts data mixture weights during fine-tuning.
                Unlike previous methods, which primarily focus on static or
                proxy-based optimization, DynaMO integrates the optimization
                process directly into the training loop. Our hypothesis is that
                this dynamic approach will lead to measurable improvements in
                training efficiency, uncover superior performance outcomes, and
                potentially reveal new principles governing data mixture
                optimization. <br /><br />
            </div>
            <div class="margin-right-block">
                Margin note that clarifies some detail #main-content-block for
                intro section.
            </div>
        </div>
        <div class="content-margin-container" id="intro">
            <div class="margin-left-block"></div>
            <div class="main-content-block">
                <h1 style="font-size: 32px">Background and Literature</h1>
                <hr style="border-top: 1px solid #cccccc" />

                The question of how to optimally mix different types of training
                data has been a persistent challenge in machine learning.
                Traditionally, researchers relied heavily on manual heuristics
                and static mixing strategies. The Llama 3.1 team's
                <a href="#ref_1">[1]</a> approach of prioritizing "high-quality"
                data in specific domains like mathematics and reasoning
                exemplifies this traditional methodology. However, this is very
                trial and error based and may not give the best result. To solve
                this, we draw inspiration from several rich areas of machine
                learning:
                <br />

                <h3 style="font-size: 20px">
                    1. Multi-armed Bandit (MAB) Theory
                </h3>

                Multi-armed bandit theory generally explores how to sample from
                a dataset with only limited information about its distribution.
                Classical MAB works <a href="#ref_2">[2]</a> provide frameworks
                for optimizing decisions under uncertainty - perfectly
                applicable to our dynamic mixing problem. Recent adaptations
                like contextual bandits <a href="#ref_3">[3]</a> have made these
                connections even more relevant.

                <h3 style="font-size: 20px">2. Curriculum Learning</h3>
                Our work shares deep connections with curriculum learning
                principles (Bengio et al., 2009). The core assumptions are
                similar:
                <ul>
                    <li>Tasks can be meaningfully ordered.</li>
                    <li>Learning transfers from simpler to complex tasks.</li>
                    <li>Training sequence impacts final performance.</li>
                </ul>
                <br /><br />

                There have been other attempts at mixture optimization and
                understanding data mixtures. Recent years have seen a surge in
                more sophisticated approaches.
                <b>DoReMi</b> <a href="#ref_4">[4]</a> introduced a particularly
                elegant solution using reference models to dynamically infer
                optimal mixture weights. The key insight was training smaller
                proxy models on different mixture combinations, then using their
                performance to predict optimal mixing strategies for larger
                models. This effectively transformed mixture weights from fixed
                hyperparameters into learnable parameters that could be
                optimized through a meta-learning approach. However, this
                approach needs predefined data mixtures and cannot be customized
                for finetuning datasets, or for more fine-graind division. It
                also assumes that data mixtures in small models generalize to
                larger models and datasets.<br /><br />
                Similarly, <b>Skill-it</b> <a href="#ref_5">[5]</a> approached
                the problem through the lens of prerequisite relationships,
                introducing a "skills graph" that maps dependencies between
                different training objectives. By modeling how different
                capabilities build upon each other, Skill-it can dynamically
                adjust data mixture weights based on the model's current skill
                proficiency and the natural progression of skill acquisition.
                This graph-based approach provides an interpretable framework
                for curriculum learning strategies. While powerful, this
                approach requires careful manual design of the skills graph and
                may not generalize well to domains where skill relationships are
                less clear.<br /><br />
            </div>
            <div class="margin-right-block">
                Margin note that clarifies some detail #main-content-block for
                intro section.
            </div>
        </div>

        <div class="content-margin-container" id="intro">
            <div class="margin-left-block"></div>
            <div class="main-content-block">
                <h1 style="font-size: 32px">Conclusions and Limitations</h1>
                <hr style="border-top: 1px solid #cccccc" />
                Let's end with some discussion of the implications and
                limitations.
            </div>
            <div class="margin-right-block">
                Margin note that clarifies some detail #main-content-block for
                intro section.
            </div>
        </div>

        <div class="content-margin-container" id="citations">
            <div class="margin-left-block"></div>
            <div class="main-content-block">
                <div class="citation" id="references" style="height: auto">
                    <br />
                    <span style="font-size: 16px">References:</span><br /><br />
                    <a id="ref_1"></a>[1]
                    <a href="https://arxiv.org/abs/2407.21783"
                        >The Llama 3 Herd of Models</a
                    >, Meta, 2024<br /><br />
                    <a id="ref_2"></a>[2]
                    <a href="https://arxiv.org/abs/1911.07676"
                        >Learning with Good Feature Representations...</a
                    >, Lattimore and Szepesvári, 2020, for example<br /><br />

                    <a id="ref_3"></a>[3]
                    <a href="https://arxiv.org/abs/1905.01435"
                        >Tight Regret Bounds for Infinite...</a
                    >, Li et al., 2021<br /><br />

                    <a id="ref_4"></a>[4]
                    <a href="https://arxiv.org/abs/2305.10429"
                        >DoReMi: Optimizing Data Mixtures Speeds Up Language
                        Model Pretraining</a
                    >, Xie et al., 2023<br /><br />
                    <a id="ref_5"></a>[5]
                    <a href="https://arxiv.org/abs/2307.14430"
                        >Skill-it! A Data-Driven Skills Framework for...</a
                    >, Chen et al., 2023<br /><br />
                </div>
            </div>
            <div class="margin-right-block">
                <!-- margin notes for reference block here -->
            </div>
        </div>
    </body>
</html>
